{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create the Ensemble with voting aggregation\n",
    "# the Autoselection porduced by SOCP controls the pruning degree of the Ensemble\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from sklearn.metrics import  accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the results of running the SOCP autoselection code\n",
    "results = io.loadmat(\"socp_output.mat\")\n",
    "result = results[\"result_train\"]\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read model output matrix\n",
    "data = io.loadmat(\"RandModel_PRED_m.mat\")\n",
    "pred = data[\"PRED_m\"]\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(result[24]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segment the data set based on the train validation split determined by SOCP process (40k samples in this example)\n",
    "segmented_first = pred[:,0:40000]\n",
    "print(segmented_first.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_last = pred[:,40000:]\n",
    "print(segmented_last.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble run with a threshold hyperparamter to specify pruning degree\n",
    "# use this section for experimentation on validation part to obtain the best subset\n",
    "experiment_eval = []\n",
    "for exp in range(result.shape[1]):\n",
    "    print(\"hyper-parameter experiment{}\".format(exp))\n",
    "    #weights = result[exp]*100\n",
    "    weights =[]\n",
    "    for model_weight in range(result.shape[0]):\n",
    "        weights.append(result[model_weight][exp]*100)\n",
    "\n",
    "    # specify pruning degree threshold\n",
    "    thresh = 0.131\n",
    "    weights = [1 if ele > thresh else 0 for ele in weights ]\n",
    "    larger_elements = [element for element in weights if element > 0]\n",
    "    number_of_elements = len(larger_elements)\n",
    "    \n",
    "    \n",
    "   # voting \n",
    "    voted_pred = [] \n",
    "    for model in range(segmented_last.shape[0]):\n",
    "        pred_temp = segmented_last[model]\n",
    "        if weights[model]:\n",
    "            voted_pred.append(pred_temp)\n",
    "    voted_pred = np.array(voted_pred)\n",
    "    # aggregation\n",
    "    aggregate = []\n",
    "    for sample in range(voted_pred.shape[1]):\n",
    "        sample_preds =[]\n",
    "        for model in range(voted_pred.shape[0]):\n",
    "            sample_preds.append(voted_pred[model][sample])\n",
    "        \n",
    "        aggregate.append(max(set(sample_preds), key = sample_preds.count))\n",
    "    # save accuracy results to further analyse on validation set   \n",
    "    acc = accuracy_score(y_train[40000:],aggregate)\n",
    "    experiment_eval.append(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation checks on validation results\n",
    "best_experiment = experiment_eval.index(max(experiment_eval))\n",
    "print(best_experiment)\n",
    "print(experiment_eval[best_experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_experiment = experiment_eval.index(min(experiment_eval))\n",
    "print(worst_experiment)\n",
    "print(experiment_eval[worst_experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_experiment = np.average(experiment_eval)\n",
    "print(average_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full Ensemble results for comparison checks\n",
    "og_model_acc = []\n",
    "for model in range(segmented_last.shape[0]):\n",
    "    pred_temp = segmented_last[model]\n",
    "    acc_temp = accuracy_score(y_train[40000:],pred_temp)\n",
    "    og_model_acc.append(acc_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(og_model_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_og = og_model_acc.index(max(og_model_acc))\n",
    "print(best_og)\n",
    "print(og_model_acc[best_og])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_og = og_model_acc.index(min(og_model_acc))\n",
    "print(worst_og)\n",
    "print(og_model_acc[worst_og])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_og = np.average(og_model_acc)\n",
    "print(average_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply this section after specifying the chosen degree of pruning (threshold)\n",
    "# saves the accuracy of the subsetted pruned Ensemble to ease comparison \n",
    "weights =[]\n",
    "for model_weight in range(result.shape[0]):\n",
    "        weights.append(result[model_weight][3]*100)\n",
    "        \n",
    "thresh = 0.131\n",
    "weights = [1 if ele > thresh else 0 for ele in weights ]\n",
    "\n",
    "larger_elements = [element for element in weights if element > 0]\n",
    "number_of_elements = len(larger_elements)\n",
    "print(number_of_elements)       \n",
    "subset_model_acc = []\n",
    "for model in range(segmented_last.shape[0]):\n",
    "    if weights[model]:\n",
    "        pred_temp = segmented_last[model]\n",
    "        acc_temp = accuracy_score(y_train[40000:],pred_temp)\n",
    "        subset_model_acc.append(acc_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subset_model_acc)\n",
    "print(len(subset_model_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_subset = subset_model_acc.index(max(subset_model_acc))\n",
    "print(best_subset)\n",
    "print(subset_model_acc[best_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_subset = subset_model_acc.index(min(subset_model_acc))\n",
    "print(worst_subset)\n",
    "print(subset_model_acc[worst_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_subset = np.average(subset_model_acc)\n",
    "print(average_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_og = np.average(segmented_last,axis=0)\n",
    "\n",
    "aggregate_og = np.ceil(aggregate_og).astype(int)\n",
    "\n",
    "\n",
    "acc_og = accuracy_score(y_train[40000:],aggregate_og)\n",
    "print(acc_og) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data section read test data and apply ensemble voting with SOCP pruning as in the methods above\n",
    "data = io.loadmat(\"RandModel_PRED_m_test.mat\")\n",
    "pred = data[\"PRED_m_test\"]\n",
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble run on test data (apply threshold parameter from validation part)\n",
    "experiment_eval = []\n",
    "for exp in range(result.shape[1]):\n",
    "    print(\"hyper-parameter experiment{}\".format(exp))\n",
    "  \n",
    "    weights =[]\n",
    "    for model_weight in range(result.shape[0]):\n",
    "        weights.append(result[model_weight][exp]*100)\n",
    "    print(len(weights))\n",
    "    \n",
    "    thresh = 0.131\n",
    "    weights = [1 if ele > thresh else 0 for ele in weights ]\n",
    "    \n",
    "    larger_elements = [element for element in weights if element > 0]\n",
    "    number_of_elements = len(larger_elements)\n",
    "    # voting\n",
    "    voted_pred = [] \n",
    "    for model in range(pred.shape[0]):\n",
    "        pred_temp = pred[model]\n",
    "        if weights[model]:\n",
    "            voted_pred.append(pred_temp)\n",
    "    voted_pred = np.array(voted_pred)\n",
    "    # aggregation\n",
    "    aggregate = []\n",
    "    for sample in range(voted_pred.shape[1]):\n",
    "        sample_preds =[]\n",
    "        for model in range(voted_pred.shape[0]):\n",
    "            sample_preds.append(voted_pred[model][sample])\n",
    "        \n",
    "        aggregate.append(max(set(sample_preds), key = sample_preds.count))\n",
    "       \n",
    "    acc = accuracy_score(y_test,aggregate)\n",
    "    experiment_eval.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain test metrics \n",
    "\n",
    "best_experiment = experiment_eval.index(max(experiment_eval))\n",
    "print(best_experiment)\n",
    "print(experiment_eval[best_experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_experiment = experiment_eval.index(min(experiment_eval))\n",
    "print(worst_experiment)\n",
    "print(experiment_eval[worst_experiment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_experiment = np.average(experiment_eval)\n",
    "print(average_experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with full Ensemble on test data\n",
    "og_model_acc = []\n",
    "for model in range(pred.shape[0]):\n",
    "    pred_temp = pred[model]\n",
    "    acc_temp = accuracy_score(y_test,pred_temp)\n",
    "    og_model_acc.append(acc_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_og = og_model_acc.index(max(og_model_acc))\n",
    "print(best_og)\n",
    "print(og_model_acc[best_og])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_og = og_model_acc.index(min(og_model_acc))\n",
    "print(worst_og)\n",
    "print(og_model_acc[worst_og])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_og = np.average(og_model_acc)\n",
    "print(average_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset on test for ease of comaprison\n",
    "weights =[]\n",
    "for model_weight in range(result.shape[0]):\n",
    "        weights.append(result[model_weight][3]*100)\n",
    "        \n",
    "thresh = 0.131\n",
    "weights = [1 if ele > thresh else 0 for ele in weights ]\n",
    "\n",
    "larger_elements = [element for element in weights if element > 0]\n",
    "number_of_elements = len(larger_elements)\n",
    "print(number_of_elements)        \n",
    "subset_model_acc = []\n",
    "for model in range(pred.shape[0]):\n",
    "    if weights[model]:\n",
    "        pred_temp = pred[model]\n",
    "        acc_temp = accuracy_score(y_test,pred_temp)\n",
    "        subset_model_acc.append(acc_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_subset = subset_model_acc.index(max(subset_model_acc))\n",
    "print(best_subset)\n",
    "print(subset_model_acc[best_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_subset = subset_model_acc.index(min(subset_model_acc))\n",
    "print(worst_subset)\n",
    "print(subset_model_acc[worst_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_subset = np.average(subset_model_acc)\n",
    "print(average_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
